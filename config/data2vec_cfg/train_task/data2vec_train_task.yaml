# @package _group_

TrainTask:

  dataset_class:
    _target_: dataset.dataset.Dataset
    path2load:
      - /home/moshelaufer/PycharmProjects/datasets/noy_synth/
    type_files: wav
    labels: True
    dataset_names: tal_noise_1

  path2load_model:

# based on: https://huggingface.co/docs/transformers/model_doc/data2vec # Data2VecVisionConfig
  model_cfg:
    num_hidden_layers: 6
    num_attention_heads: 6
    intermediate_size: 2048
    image_size: 32
    num_channels: 3
    hidden_act: gelu
    hidden_dropout_prob: 0.0
    patch_size: 16

  # training param
  epochs: 4000
  path2save_model: C:\Users\moshe\PycharmProjects\checkpoints\ssl
  model_name: data2vec_

  steps_per_epoch: 10
  validation_steps: 10

  batch_size:
    train: 128
    test: 1
    valid: 128

  # optimizer
  optimizer:
    _target_: tensorflow.keras.optimizers.Adam
    learning_rate: 1e-4

  schedule:
    _target_: callbacks.WarmLRSchedule
    initial_learning_rate: 6e-6
    warmup_steps: 8
    hold_step: 1000 #1
    decay_step: 10
    max_learn_rate: 7.5e-5
    min_learn_rate: 1.5e-6

  # losses
  loss:
    - _target_: Projects_torch.losses.l2_loss.L2Loss

  # callbacks
  callbacks:
    -
      _target_: tensorflow.keras.callbacks.ModelCheckpoint
      filepath: /home/moshelaufer/PycharmProjects/results/checkpoint/synth_autoencoder_noy/8/
      save_best_only: True
      save_freq: 7500 #'epoch'
      initial_value_threshold: 5
      monitor: 'linear_classifier_loss'
      verbose: 1

  # dataloader
  processor:
    _target_: Projects_torch.processors.processor_synth_encoder_eml_spec.DataLoaderMelSpec
    autoencoder: False
    encoder: True

  num_workers: 2